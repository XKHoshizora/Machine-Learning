# 机器学习

[TOC]

# 1 机器学习概论

## 1.1 机器学习的定义

什么是机器学习（Machine Learning）？
	
- 定义一：

	Machine Learning is Fields of study that gives computers the ability to learn without being explicitly programmed.
	（机器学习是指，不通过显著式编程而使计算机获得学习的能力的一个领域。）
										—— 1959年，Arthur Samuel
	
	- 非显著式编程
		
		让计算机自己总结规律的编程方法，叫做非显著式编程。
		
		最大化收益函数

- 定义二：

	A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.
	（一个计算机程序被称为可以学习，是指它能够针对某个任务 T 和某个性能指标 P，从经验 E 中学习，这种学习的特点是，它在 T 上的被 P 所衡量的性能，会随着经验 E 的增加而提高。）
										—— 《Machine Learning》Tom Mitshell

## 1.2 机器学习的分类

按照任务**是否需要和环境交互获得经验**，机器学习可分为**监督学习**和**强化学习**。

- 监督学习（Supervised Learning）

	（1）根据**训练数据是否有标签**，监督学习可分为**传统的监督学习**、**非监督学习**和**半监督学习**。

	- 传统的监督学习（Traditional Supervised Learning）

		```
		每一个训练数据都有对应的标签。
		```
		
		算法：
		
		1. 支持向量机（Support Vector Machine）
		2. 人工神经网络（Neural Networks）
		3. 深度神经网络（Deep Neural Networks）
	
	- 非监督学习（Unsupervised learning）
	
		```
		所有的数据都没有对应的标签,
		只需要定义收益函数（Reward Function）。
		```
		
		算法：
		
		1. 聚类（Clustering）
		2. EM算法（Expectation-Maximization Algorithm）
		3. 主成分分析（Principle Component Analysis）
	
	- 半监督学习（Semi-supervised Learning）
	
		```
		训练数据中，一部分有标签，一部分没有标签。
		研究如何通过少量的标注数据和大量的未标注数据，训练更好的机器学习算法。
		```
		
	（2）根据标签是**连续**还是**离散**，将监督学习分为**回归问题**和**分类问题**。但回归和分类问题的界线非常模糊。
	
	- 分类（Classification）

		```
		标签是离散的值。
		```

	- 回归（Regression）

		```
		标签是连续的值。
		```

- 强化学习

  ```
  计算机通过与环境的互动，逐渐强化自己的行为模式。
  ```
  
## 1.3 机器学习算法的过程

	注意：
	机器学习的重点不是研究如何提取数据特征，
	而是假设在已经提取好特征的情况下，如何构造算法获得更好的性能。
	但这并不是说特征提取不重要，反而是非常重要，会直接影响性能的好坏。
	
	为什么不重点研究特征提取呢？
	因为不同的任务，提取特征的方式不同，甚至千变万化。

步骤：

1. 特征提取（Feature Extraction）

	```
	通过训练样本获得的，对机器学习任务有帮助的多维度数据。
	```
	
2. 基于特征构建算法
	- 支持向量机（Support Vector Machine）的三个内核（算法）：
		- 线性内核
		- 多项式内核
		- 高斯径向基函数核
		
		
		
3. 获得结果

	关键词：
	- 特征空间（Feature Space）
	- 维度：人类缺乏对三维以上世界的想象力。
	- 标准：算法不同，对某些相同区域的划分不同，所以我们不能得出绝对意义上好和坏的标准。

## 1.4 没有免费午餐定理

> **没有免费午餐定理（No Free Lunch Theorem）**
> （1995年，D.H.Wolpert 等人提出）
> 
> 任何一个预测函数，如果在一些训练样本上表现好，那么必然在另一些训练样本上表现不好。如果不对数据在特征空间的先验分布有一定的假设，那么表现好与不好的情况一样多。

在设计机器学习算法的时候，有一个假设：

	在特征空间上距离接近的样本，他们同属于一个类别的概率会更高。

这个假设有道理吗？

	道理是从以前的事实中来的，通过类比，推广到对未来的预测。

这样的预测是否可能出错？

	很可能！

# 2 支持向量机（Support Vector Machine）

理论框架提出者：Vladimir Vapnik（俄罗斯）

- 线性可分（Linear Separable）

	指可以用一条直线，将样本分隔开。

- 线性不可分（Nonlinear Separable）

	指不存在可以将样本分隔开的直线。

- 二维空间用直线，三维空间用平面，四维及以上空间使用超平面（Hyperplane）分割

## 2.1 线性可分定义

- 线性可分的严格定义：一个训练样本集 $\{(X_i, y_i), ..., (X_N, y_N)\}$，在 $i=1 \sim N$ 线性可分，是指存在 $(\omega_1, \omega_2, b)$，使得对 $i=1 \sim N$，有：
	
	（1）若 $y_i=+1$，则 $\omega_1x_{i1}+\omega_2x_{i2}+b>0$
	
	（2）若 $y_i=-1$，则 $\omega_1x_{i1}+\omega_2x_{i2}+b<0$

- 用向量形式定义线性可分

	假设：
		$$X_i=\begin{bmatrix}x_{i1} \\ x_{i2}\end{bmatrix}^T，\omega=\begin{bmatrix}\omega_1 \\ \omega_2\end{bmatrix}^T$$

	（1）若 $y_i=+1$，则 $\omega^Tx_i+b>0$
	
	（2）若 $y_i=-1$，则 $\omega^Tx_i+b<0$
	
- 线性可分定义的最简化形式
	
	如果
		$$y_i=+1 或 -1$$
		
	一个训练样本集 $\{(X_i, y_i)\}$，在 $i=1 \sim N$ 线性可分，是指存在 $(\omega, b)$，使得对 $i=1 \sim N$，有：
		$$y_i(\omega^TX_i+b)>0$$

## 2.2 问题描述

	如果一个数据集是线性可分的，则存在无穷多个超平面将各个类别分开。（可证）
		
支持向量机算法

1. 解决线性可分问题；
2. 再将线性可分问题中获得的结论推广到线性不可分情况。

如何解决线性可分问题？

- 在这无数多个分开各个类别的超平面中，到底哪一个最好呢？—— Vladimir Vapnik
	
	基于最优化理论
	
	![图1](/Users/hoshizora/Pictures/Photos Library.photoslibrary/resources/derivatives/9/972AD40F-0812-45F8-82F6-CB9293686C67_1_102_o.jpeg)
	
	与两个平行线接触的样本称为**支持向量**，两平行线之间的距离称为**间隔**。
	
	使间隔（Margin）最大的线，即是要找的线（不唯一），且应在两条平行线的正中间（唯一）。
	
- 支持向量机寻找的最优分类直线（超平面）应满足：
	1. 该直线（超平面）分开了两类；
	2. 该直线（超平面）最大化间隔（Margin）；
	3. 该直线（超平面）处于间隔的中间，到所有支持向量距离相等。
	
	证明：在线性可分的条件下，有且只有一条直线（超平面），满足以上三个条件。
	
## 2.3 优化问题	

	如何用严格的数学，将寻找最优分类超平面的过程，写成一个最优化问题？
	
假定训练样本集是线性可分的，支持向量机需要寻找的是最大化间隔（Margin）的超平面。
	
最小化（Minimize）：$\frac{1}{2}\begin{Vmatrix}\omega\end{Vmatrix}^2$

限制条件：$y_i(\omega^Tx_i+b)\geq1，(i=1 \sim N)$

- 事实1

	$\omega^Tx+b=0$ 与 $(a\omega^T)x+ab=0$ 是同一个超平面。$(a\neq0)$
	
- 事实2

	一个点 $X_0$ 到超平面 $\omega^Tx+b=0$ 的距离
	$$d=\frac{\begin{vmatrix}\omega^Tx_0+b\end{vmatrix}}{\begin{Vmatrix}\omega\end{Vmatrix}}$$
	
> 支持向量机优化问题推导中最难理解的部分：
> 
> 用 $a$ 去缩放 $\omega b$
> $$(\omega, b)\rightarrow(a\omega, ab)$$
> 最终使在支持向量 $x_0$ 上有 $\begin{vmatrix}\omega^Tx_0+b\end{vmatrix}=1$，
> 而在非支持向量上 $\begin{vmatrix}\omega^Tx_0+b\end{vmatrix}>1$。
> 
> 根据事实2，支持向量 $X_0$ 到超平面的距离将会变为：
> $$d=\frac{\begin{vmatrix}\omega^Tx_0+b\end{vmatrix}}{\begin{Vmatrix}\omega\end{Vmatrix}}=\frac{1}{\begin{Vmatrix}\omega\end{Vmatrix}}$$
> 也就是说，最大化支持向量到超平面的距离 $\Longleftrightarrow$ 最小化 $\begin{Vmatrix}\omega\end{Vmatrix}$。
> 
> 则可将优化问题定为：
> 最小化 $\frac{1}{2}\begin{Vmatrix}\omega\end{Vmatrix}^2$，也就是最小化 $\begin{Vmatrix}\omega\end{Vmatrix}$
> 
> 限制条件：$y_i(\omega^Tx_i+b)\geq1 \quad i=1 \sim N$
> 
> 其中，$y_i$ 用于协调平面的左右，不等式右边的 1 可改为任意正数（事实1）。
> 
> 因此，线性可分的情况下，支持向量机寻找最佳超平面的优化问题可以表示为：
> 
> - 凸优化（Convex Optimization）
> 
> 		最小化（Minimize）：$\frac{1}{2}\begin{Vmatrix}\omega\end{Vmatrix}^2$
>
> 		限制条件：$y_i(\omega^Tx_i+b)\geq1，(i=1 \sim N)$
> 
> 		$(X_i, y_i), i=1 \sim N$ 是已知的，$(\omega, b)$ 是待求的

- 二次规划定义：

	（1）目标函数（Objective Function）是二次项。
	
	（2）限制条件是一次项。
	
		要么无解，要么有唯一的最小值。
		
在最优化理论中，如果一个问题是凸优化问题，我们就会把它当作已经解决的问题，因为凸优化问题只有唯一一个全局极值。

	一个优化问题是凸的，则总能找到高效快速的算法去解决它。
	机器学习不详细讨论如何求解凸优化问题，而只是假定把一个问题转化为凸优化问题之后，就能使用解凸优化问题的算法工具包方便地解出。
	求解凸优化问题是一门专门的课程。（《凸优化理论》）

## 2.4 线性不可分情况
	
如果训练样本集是线性不可分的，则在如下限制条件下无解：
> 最小化（Minimize）：$\frac{1}{2}\begin{Vmatrix}\omega\end{Vmatrix}^2$
> 
> 限制条件：$y_i(\omega^Tx_i+b)\geq1，(i=1 \sim N)$

即不存在 $\omega$ 和 $b$ 满足上面所有 $N$ 个限制条件。

因此，对于线性不可分的情况，需适当放松限制条件，使得最优化问题有解。放松限制条件的基本思路：
> 对与每一个训练样本及标签 $(x_i, y_i)$ 我们需要设置一个松弛变量 $\delta_i$（Slack Variable）：

> 限制条件改写为：$y_i(\omega^Tx_i+b)\geq1-\delta_i，(i=1 \sim N)$

改造后的支持向量机优化版本：

最小化（Minimize）：$\frac{1}{2}\begin{Vmatrix}\omega\end{Vmatrix}^2+C\sum_{i=1}^N\delta_i$ 或 $\frac{1}{2}\begin{Vmatrix}\omega\end{Vmatrix}^2+C\sum_{i=1}^N\delta_i^2$，其中 $C$ 是人为设定的比例因子

限制条件：$(1)\;\delta_i\geq0，(i=1 \sim N) \\ (2)\;y_i(\omega^Tx_i+b)\geq1-\delta_i，(i=1 \sim N)$

> 需要事先人为设定的参数叫做算法的超参数（Hyper Parameter）。一般，在实际的应用中，需不断变化 $C$ 的值，同时对每个 $C$ 测试算法对识别率，然后选取使识别率达到最大的超参数 $C$ 的值。
> 
> 支持向量机是超参数很少的算法模型。

## 2.5 低维到高维的映射

支持向量机在扩大可选函数范围方面，可谓是独树一帜。其他算法都是直接产生更多的可选函数。如：决策树、人工神经网络中的多层非线性函数的组合等。

支持向量机不是直接产生可选函数，而是通过将特征空间由低维映射到高维，然后在高维的特征空间中，仍然用线性超平面对数据进行分类。

> 定理：
> 
> 假设1:
> 在一个 $M$ 维空间上随机取 $N$ 个训练样本，并随机对每个训练样本赋予标签 $+1$ 或 $-1$。
> 
> 假设2:
> 假设1里的训练样本线性可分的概率为 $P(M)$。
> 
> 则当 $M$ 趋于无穷大时，$P(M)=1$。

关键问题：如何构造由低维到高维的映射 $\varphi(X)$ ?

假设 $\varphi(X)$ 已经确定，改造后的支持向量机优化版本：

最小化（Minimize）：$\frac{1}{2}\begin{Vmatrix}\omega\end{Vmatrix}^2+C\sum_{i=1}^N\delta_i$ 或 $\frac{1}{2}\begin{Vmatrix}\omega\end{Vmatrix}^2+C\sum_{i=1}^N\delta_i^2$，其中 $C$ 是人为设定的比例因子

限制条件：$(1)\;\delta_i\geq0，(i=1 \sim N) \\ (2)\;y_i[\omega^T\varphi(X_i)+b]\geq1-\delta_i，(i=1 \sim N)$

低纬度下，$\omega$ 维度与 $X_i$ 维度相同；高维度下，$\omega$ 维度与 $\varphi(X_i)$ 维度相同。

## 2.6 核函数的定义

具体研究 $\varphi(x)$ 的形式，并以此为基础引入核函数（Kernel Function）。

可以不用知道 $\varphi(x)$ 的具体形式。如果对任意两个向量 $X_1$ 和 $X_2$，已知 $$K(X_1, X_2)=\varphi(X_1)^T\varphi(X_2)$$ 的情况下，仍可以通过技巧，获得测试样本的类别信息，从而完成对测试样本类别的预测。

我们定义 $K(X_1, X_2)$ 为核函数（Kernel Function），该函数为一个实数。

核函数 $K$ 和映射 $\varphi$ 是一一对应的关系，知其一则可求另一个。

核函数的形式不能随意取，需满足一定的条件才能分解成两个 $\varphi$ 内积的形式。

> Mercer's Theorem
> 
> $K(X_1, X_2)$ 能写成 $\varphi(X_1)^T\varphi(X_2)$ 的充要条件为：
> 
> 1. $K(X_1, X_2)=K(X_2, X_1)$ （交换性）
> 2. $\forall C_i(i=1 \ sim N),\forall N$ 有 $\sum_{i=1}^N\sum_{j=1}^NC_iC_jK(X_i, X_j)\geq0$ （半正定性）

## 2.7 原问题和对偶问题

一个优化问题的原问题（Prime Problem）与其相对应的对偶问题（Dual Problem）的定义如下：
> - 原问题（Prime Problem）：
> 
> 		最小化（Minimize）：$f(\omega)$
> 
> 		限制条件（Subject to）：$g_i(\omega)\leq0,i=1 \sim K \\ h_i(\omega)=0,i=1 \sim m$
> 
> - 上面原问题的对偶问题（Dual Problem）：
> 		- 定义函数：
> 			$$\begin{align}L(\omega, \alpha, \beta)&=f(\omega)+\sum_{i=1}^K\alpha_ig_i(\omega)+\sum_{i=1}^M\beta_ih_i(\omega) \\ &=f(\omega)+\alpha^Tg(\omega)+\beta^Th(\omega)\end{align}$$
> 			其中
> 			$$\begin{align}&\alpha=[\alpha_1, \alpha_2, ..., \alpha_K]^T \\ &\beta=[\beta_1, \beta_2, ..., \beta_M]^T \\ &g(\omega)=[g_1(\omega), g_2(\omega), ..., g_K(\omega)]^T \\ &h(\omega)=[h_1(\omega), h_2(\omega), ..., h_M(\omega)]^T\end{align}$$
> 
> 		- 在定义了函数 $L(\omega, \alpha, \beta)$ 的基础上，对偶问题如下：
> 
> 			最大化：$\theta(\alpha, \beta)=inf\;L(\omega, \alpha, \beta)$，所有定义域内的 $\omega$
> 
> 			限制条件：$\alpha_i\geq0,i=1 \sim K$
> 
> - 综合原问题和对偶问题的定义得到：
> 		- 定理一：
> 
> 			如果 $\omega^*$ 是原问题的解，$(\alpha^*, \beta^*)$ 是对偶问题的解，则有：
> 			$$f(\omega^*)\geq\theta(\alpha^*, \beta^*)$$
> 
> 		- 对偶差距（Duality Gap）
> 
> 			$$f(\omega^*)-\theta(\alpha^*, \beta^*)$$
> 			由定理一可知，$f(\omega^*)-\theta(\alpha^*, \beta^*)\geq0$。
> 
> 		- 强对偶定理（Strong Duality Theorem）
> 
> 			如果 $g(\omega)=A\omega+b$，$h(\omega)=C\omega+d$，$f(\omega)$ 为凸函数，则有 $f(\omega^*)=\theta(\alpha^*, \beta^*)$，则对偶差距为 $0$。
> > 
> > 如果原问题的目标函数是凸函数，限制条件是线性函数，则 $f(\omega^*)=\theta(\alpha^*, \beta^*)$，即对偶差距为 $0$。
> 
> 		- 据定理一推出的不等式得出
> 
> 			若 $f(\omega^*)=\theta(\alpha^*, \beta^*)$，则定理一中必然能够推出，对于所有的 $i=1 \sim K$，要么 $\alpha_i=0$，要么 $g_i(\omega)=0$。这个条件被称为KKT条件

## 2.8 转化为对偶问题

